---
title: "rough overview"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

`r Sys.Date()`

## Notation

- $x$ is some underlying cognitive process of interest that will implicity be understood to be a function of time, i.e., $x \equiv x(t)$
- $H(x) = \mathcal{Y}$ represents theoretical eye movement curve (logistic, double gauss)
- Because of saccades, overlapping phonetics, extrogenous/introgenous variables, we actually observe
$$ h(\mathcal{Y}) =z$$
where $h$ is some function dependent on a potentially infinite number of exogenous/endogenous variables
- Letting $z_{ijt} = \{0,1\}$ represent subject $i = 1, \dots, n$, trial (single instance of vwp) $j = 1, \dots, J$, at time $t = 1, \dots, T$,  we define

$$
y_{it} = \frac{1}{J} \sum_{j=1}^J z_{itj}
$$

- where $y_{it}$ represents the *proportions of fixations* towards a target for a particular individual at time $t$
- where we might call $z$ the unaggregated data, and $y$ the aggregated data for each subject. OR, it may be simpler to call $z$ a *looking curve* (as it indicates whether a look is occuring or not), whereas $y$ could be a *fixation curve*, defined as proportion of fixations of a target over a number of trials.
- the present goal of bdots is to construct a function $f_{\theta}$ such that $f_{\theta} = \mathcal{Y}$


# bdots process

## Fitting Step

Given a subject's aggregated looks a cross trials, denoted $y_{i} \equiv y_{i\cdot}$ and a hypothetical underlying curve $f_{\theta}$ (doublegauss, logistic), governed by parameters $\theta_f$ (though we will assume constant $f$ and drop that notation), the `bdotsFit` function returns sufficient statistics for the distribution of the curve parameters

$$
F: \{y\} \times f \rightarrow N(\hat{\theta}_i, \hat{\Sigma}_{\theta_i})
$$
where 
$$
\hat{\theta}_i = \text{argmin}_{\theta} \  ||y_{it} - f(t|\theta)||^2
$$
This step also specifies the groups that make up the subjects. These could be different populations, experimental conditions, or both. For simplicity, we could let $g = 1, \dots, G$ denote the number of unique permutations of groups that are fit. For example, if there are two populations and two experimental conditions, we would have a total of four groups. Two groups, say $g_1, g_2$, may be correlated if they come from the same population but different experimental conditions, i.e., if subject $i$ is in both $g_1$ and $g_2$. 

However, since this is only used in the final analysis step, we will omit any notation indicating groups. We will further assume that each subject is in one group, and that the groups are uncorrelated (although a slight aside is given in the bootstrap step)

## Bootstrap Step

`bdotsBoot` will perform $B$ bootstraps of the subject parameters and use this to construct bootstrapped curves with confidence intervals. The bootstrapping happens as follows.

1. For each subject, we will draw $B$ samples of $\hat{\theta}_i$. Note here that if a subject is in $g$ groups, we will draw $g$ sets of the parameters at once. For example, if a subject is in two groups, and $\hat{\Sigma}_g$ represents the computed covariance matrix between them, we would be drawing from

$$
\begin{bmatrix} \hat{\theta}_{ib1} \\ \hat{\theta}_{ib2} \end{bmatrix} 
\sim 
N \left(
\begin{bmatrix} \hat{\theta}_{ib1} \\ \hat{\theta}_{ib2} \end{bmatrix} ,
\begin{bmatrix} \hat{\Sigma}_{\theta_{i1}}  & \hat{\Sigma}_g \\ \hat{\Sigma}_g^T & \hat{\Sigma}_{\theta_{i1}} \end{bmatrix} 
\right)
$$
For simplicity, we will assume for now they are not correlated, and we are drawing from $N(\hat{\theta}_i, \hat{\Sigma}_{\theta_i})$. The result will be a $B \times p$ matrix, which is $B$ bootstraps of the $p$ parameters. We will denote this matrix $M_{\hat{\theta}_{i}}$.

2. Once this is done for each subject, we will aggregate all of the parameter estimates letting this aggregated parameter matrix be

$$
\overline{M}_{\hat{\theta}} = \frac{1}{n} \sum_{i=1}^n M_{\hat{\theta}_{i}}
$$

where $\overline{M}_{\hat{\theta}}$ is a $B \times p$ matrix, each row representing the average parameter estimate of $\theta$ across subjects at each bootstrap $b$.

3. We then use the the parameters in each row to reconstruct an estimate of $f$ , which represents the theoretical fixation curve for each population/group. Therefore, each $1 \times p$ row of $\overline{M}_{\hat{\theta}}$ returns a $1 \times T$ row of proportion of fixations at each time point. We end with a $B \times T$ matrix, denoted $\overline{M}_{f}$

4. From this, we get a final estimate of the "looking" curve, 

$$
\hat{f} = \frac{1}{B} \sum_{b=1}^B \overline{M}_{\{i, \cdot\}_{f}} \qquad \widehat{\text{se}}_{f} = \left[ \frac{1}{B-1} \sum_{b=1}^B \left( \overline{M}_{\{i, \cdot\}_{f}} - \hat{f} \right)^2 \right]^{1/2} 
$$
where each of these is a vector of length $T$, each element corresponding to a time point. 

# Discussion

A critical shortcoming of this methods involves the calculation of each subjects aggregated looks, expressed as a proportion. For this discussion, and for notational purposes, we are only considering a single subject, with eye-tracking measured at $t = 1, \dots, T$. At present, each time point is separated by 4ms, and so $t$ should be understood to be an index, rather than actual time.

Recall for VWP that the four images are made up of 

1. Wizard -- target
2. Lizard -- rhyme
3. Whistle -- cohort
4. Monkey -- unrelated

We will assume wolg that we are considering looks to the target. Within a given trial of the VWP, $Z_t \in \{0,1\}$ indicates if a subjects eyes are focused in some prescribed window of the target. We may assume that $Z_t \sim Bern(p_t)$. There are a number of factors that may go into $p_t$:

1. Subject specific
- time point (more likely to settle on correct image after partial/full resolution of word)
- cognitive processing speed
- hearing
- location of previous fixation. Although eye movements are measured psuedo-continuously, in reality it takes about 200ms to identify a location and make the subsequent eye movement. More on this below. 

2. Trial specific
- timing of phonetic overlap between competitors
- semantic/phonetic competition

### "Look intervals"

As a consequence of the fact that my eyes are not actually in continuous movement, it makes some sense to talk about "look intervals" of stochastic length: if I *start* fixating on a particular location at time $t$, it is highly likely that I will be fixating there at $t+1$. We might then be interested, instead of considering specific time points, considering specific *intervals*. I'm running out of indices, but we might describe an interval 

$$
I_{t, k} = \{z_j = 1 \ | \ t \leq j \leq t+ k \}, \qquad \overline{I}_{t, k} = \{z_j = 0 \ | \ t \leq j \leq t+ k \}
$$
We could then represent any individual trial as an ordered sequence of intervals, the length of which is given by the number of time points included. This length would then be represented by a negative binomial/hypergeometric...something to account for the fact that the larger the interval, the more likely it is to 'break' at the next given time point.


### Constructing the curve

We are fundamentally interested in learning about some unobserved, latent variable $x$ that describes a cognitive process. In particular, we are interested in learning about this variable as a process over time. In dynamical systems literature, this is usually expressed $\dot{x}$ or perhaps $x(t)$, but we will use $x$ instead, understanding that this is a function of time, making the reference to time explicit only when it is directly relevant

We might consider some function $H$ such that $H(x) = \mathcal{Y}$, where $\mathcal{Y}$ is a fictional "looking curve" (doublegauss/logistic). I say fictional as any specific instance of a looking curve is necessarily dependent on exogenous variables (phonetics, image representation, etc.). A specific instance of a looking curve, governed by a set of parameters $\theta$, could be expressed $f_{\theta}$ For ease of discussion, we might call the collection of exogenous variables associated with a particular instance $W$, and say that

$$
H(x) = \mathcal{Y}, \qquad H(x|W) = f_\theta.
$$
However, this is not what we *actually observe*. Let $G$ be some function that governs the actual mechanics of eye movements (causes them to jitter, launch saccades, move in discerete intervals). We imprecisely measure $G$ by simply indicating if eyes are fixated on a particular location or not. What is actually observed, then, is

$$
\tilde{G} \circ H(x(t)|W) = z_t
$$
and we approximate $\tilde{G}$ indirectly (or rather, attempt to work around it) by performing $J$ trials and aggregating over each time point, 

$$
y_{t} = \frac{1}{J} \sum_{j=1}^J z_{tj}
$$
In aggregate, this looks like

<!-- \begin{align} -->
$$
H(x) = \mathcal{Y} \approx y_t = \frac{1}{J} \sum_{j=1}^J z_{tj} = \frac{1}{J} \sum_{j=1}^J \tilde{G} \circ H(x(t)|W_j) 
$$
<!-- \end{align} -->

There are a number of things at this point that are worth summarizing/noting:

1. Virtually everything above should be considered a stochastic process
2. $H$ is defined as the function that maps a cognitive process of interest to a behavioral response. Based on the interpretation of model parameters from `bdots`, this appears to be more or less the identity function insomuch as slopes are associated with word activation, sustained fixations indicate finished activtation, etc. This is probably fine for now
3. Related to point above, it almost certainly the case that $G$, the actual eye movements, are also governed by the set of exogenous variables, $W$. That is probably fine for now, and since we have a lot of work to do before we can concern ourselves with $H$ directly (needing to correctly reconstruct eye movements first), we could specify $V(x|W) = G \circ H(x|W)$

### Interpreting the curves

At first glance, one might think there is some issue with the way we are bootstrapping curves and what exactly we are making inference on. In the previous section, I noted that 

$$
H(x) = \mathcal{Y}
$$

was a fictional curve, necessarily dependent on a set of exogenous variables to become realized, yet ultimately we are looking for "population" curves to compare. That is, given populations with cognitive processes $x_1, x_2$, we are ultimately interested in comparing (as proxy) the fictional curves $\mathcal{Y}_1$ and $\mathcal{Y}_2$. Independent of the actual eye movements, we are kind of integrating the exogenous variables out with 

$$
H(x) \approx \frac{1}{J} \sum_j H(x|W_j).
$$
The same could probably be said for $G$, given a large enough $J$. However, theBob demonstrated through simulation (a paper I need to revist) that even under a number of different assumptions regarding eye mechanics, the resulting parameters were biased from the "true" generating curve, which begs the question of whether or not it makes sense to try to estimate a curve like this anyways.


## Going forward

There are a few interesting steps to go in from here.

1. On one hand, relating the behavioral response to underlying cogntive process is interesting. However, to do so successfully is predicated on having a correct model of behavioral reponse to begin with, which we don't
2. We should be using within-trial measures of $z_{tj}$ between subjects to somehow account of exogenous variables
3. Modeling the mechanics of eye movement with stoachastic intervals/binomials/etc seems like pretty low hanging fruit for this problem
4. Akin to Bob's idea, maybe it does make sense to try to find something akin to a "generating curve", though something like distributions that represent the populations (where $f_{\theta}$ would be drawn from population distribution). 

#### some thoughts (random)

instead of fitting that glm curve and estaimting parameters what we do with constraints, why don't we have starting parameters 

theta \sim {collection of different distributions}
- use bayesian priors to choose start, measure the fit, update start
- i.e., instead of gradient decent, operate under bayesian paradigm. (trunkated distributaions ok too)
- could set b0 = direct 0, could have slope be normal

- soon will be able to incorporate to other portions of variance (within looks and across trials)
- key here, *really* is to ask two questions
  1. can we assume $\hat{\theta}$ unbiased for $\theta$?
  2. If yes, how do we estimate that with minimal bases
- note, what we are really looking to compare is $\mathcal{Y}$, whatever that is. maybe we can define it somehow, but should have some connection (in structure) to whatever. This is what assocaites the top down, information in, 
- does it make sense to define $H(x) = \mathcal{Y}$ this is the pure, infinitely regressive dynamics on the human mind. It would be impossible to incorporate, we only know that, conditioned on some set $W$, we can make estiamte to it's value (acknowleding a large degree of missing information). this could possibly be suplemented with  eeg data, that is, to further  reduce $\mathcal{Y}$ to things we can measure in parallel. again, reduce variance. 
- starting to reread bob's paper, i think a lot of what he is demonstrating can be expressed concisely with mathematical statements. AND, once we have a language in which to unambiguously operate, we can make deductive statements over inductive statements, and that is much closer to true knowledge

it would be cool to have this discussion in like appendix of thesis. chapter 5 or whatever

no look, here's the thing: 

1. fit set of parameters to curve
2. compare with loss function L against distribution of $\mathcal{Y}$
  - this may include maximizing posterior, given iterative data set to loss functions
  - loss functions could be multi-part:
    - assumes general saccade movements given range of W for subject (within subject var)
    - given THAT information, we can update information associated with randomness of trial. that is, take the collection of trials, remove what was found to be associated with within subject variantion (interval lengths, delay, etc), and regress on the residuals to get an estimate of what is assoiciated with between subject information. 
    - This maybe gets tricky. But once you have estimated distribution of parameters, one capturing within subject variation and the other capturing between subject variation, possibly a third to capture structured noise (phenotype information?) that would honestly probably be better. From these however, we consider a [convolution](https://en.wikipedia.org/wiki/Convolution) that can be done in time
    - Neat thing to follow up with on that, [Kalman filter](https://en.wikipedia.org/wiki/Kalman_filter). i need to learn more about it first.
    - But i really think by setting things up this way is the way to go. This is ALMOST basically EM algorithm. the steps are the same, but instead of maximizing log likelihood, we are going to maxmize some convolution of the way the within subject/between trial/phonetic information. but like, we know the phonetic information and can add that directly. 
3. here are some great facts about that
  1. convolusion defines a product on the linear space of integrable functions (wikipedia), so it itself is an algebra, and we can do things with those bitches. And by its an algebra, they mean its a sigma algebra, and that shit means statistics. nice bit is that they can be composed in any order

looking at kalman filter, they have  linear kalman filter and they have general nonlinear ones. Given its also  proximity to bayesian analysis, it's basically what i am describing in other words. this will potentially be a great connection to have made (i should investigate because obviously it has somewhere). but anyways. kalman filter assumes additive. but perhaps it only just means linear, and i can replace addition with convolution. 

question on markov property. 

lets say all time points highly correlated, that is, we need

$P(x_n | x_1, \dots, x_{n-1}) = P(x_n | x_{n-1})$

is this true if value of x_n recusirvely dependent on its priors? so in a sense, we really only need $x_{n-1}$, but it captures all the information of the prior?

Re  kullback-leibler divergence -- if ultimately what we are deducing between two groups are two separate probability functions, each containing unknown components, we might ask, in lieu of the pieces missing for both, which of these conveys the most information (and possibly when this occurs)? This question relates directly to the general questions in psycholinguistics. which set of cognitive systems require the least amount of information to be processed
