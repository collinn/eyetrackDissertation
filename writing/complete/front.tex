% This file specifies information specific to your thesis, such as your
% title, advisor, dedication, etc.


% To remove optional components, comment out the line
\abtitlepgtrue
\abstractpgtrue
\titlepgtrue
\copyrighttrue %(optional)
%\signaturepagetrue %remove signature page
\acktrue %(optional)
\tablecontentstrue
\tablespagetrue
\figurespagetrue

\title{What You See is What You Get: A Closer Look at Bias in the Visual World Paradigm}
\author{Collin Nolte}
\advisor{Professor Patrick Breheny}
\dept{Biostatistics}
\submitdate{May 2023}
\supervisor{Patrick Breheny}
\membera{Jacob Oleson}
\memberb{Bob McMurray}
\memberc{Grant Brown}
\memberd{Kristi Hendrickson}


\newcommand{\abstextwithesis}
{ % main abstract
In 1995 the Visual World Paradigm was first introduced as an experimental paradigm relating eye-tracking data to lexical activation. This was done by measuring the location of a participant's visual fixation in real time in response to a spoken word. Shortly following this was the introduction of the ``proportion of fixations" method, whereby indicators of fixations in the VWP were aggregated across trials at dense time slices, creating an ostensible curve demonstrating the proportion of trials in which participants were fixated on particular objects at each time. In 2017, methods were introduced to make temporal analysis of these curves tractable via bootstrapping and a novel $\alpha$ correction to counteract the multiple comparison problems implicit in densely sampled time series. 

This dissertation improves upon the field in multiple ways. First, we reintroduce the \xt{bdots} package with a dramatically simplified user interface. Most prominent among these changes include the ability for users to create and fit parametric functions independent the \xt{bdots} software as well as the introduction of permutation testing for identifying temporal differences between groups. We also identify and correct methodological issues found in the original bootstrapping algorithm that, when unaccounted for, potentially lead to family-wise error rates of over 90\%. Both this correction and the newly introduced permutation test demonstrate quality maintenance of the FWER across a robust collection of underlying assumptions without significant losses in power. 

And finally, we propose a new generative model that links eye mechanics to lexical activation, along with a novel ``look onset" method that seeks to replace the proportion of fixations method. We demonstrate asymptotic consistency between our generative model and the look onset method, both with regards to the recovery of subject-specific activation curves, as well as with the identification of systematic temporal differences between groups. We conclude by demonstrating this utility with data collected from prior studies as well as by providing a number of avenues for future research. 
}



 
\newcommand{\acknowledgement}
{

% intro

% brother
%To my brother,

% to mitchell
%To Mitchell, pushing me towards greatness. explore and challenge ourselves. a faithful companion for cold coors light

% to anna
%To Anna, desire for goodness and relentless compassion

% to connor

% Advisor
To Patrick, for suffering the unique privilege of serving as my advisor and mentor, [and with whom i spent the six wholesome years i was associated with the department of biostat] for six wholesome years I spent associated with the department of biostat. Who challenged me to seek excellence, to approach the world with openness and  a genuine curiosity, and to dispassionately pursue the truth. And who showed me that no matter how much work there was to be done, that there was always time to talk about R.

% Pake
To Pake, the iron from which I crafted the most intricate/intimate parts/aspects of my character. Who shared with me the importance of family, the strength in community, and the fruits of true brotherly love. I admire His commitment to excellence, humility in the light of success and  total embrace of the gift of life. sometimes i ask myself, would pake do this thing? and if so, i do this thing. 


% Parents
To my parents, Sean and Amy, whose enduring commitment to one another and to our family created a space of safety, encouragement, and exploration. showered their children with selfless love and sacrifice. To my mother, whose unwavering and unconditional support was a source of constant strength, even when it felt as if all were lost. And to my father, whose diligence and tireless commitment to duty guided me in establishing my own priorities. Who showed me resolve in the face of difficulty and the joys of living an honorable life. 


% Dog
And finally, to Vixen. Of all the bitches, you will always be my favorite \ensuremath\varheartsuit


}

\newcommand{\pubabstextwithesis}
{ % public abstract
In 1995 the Visual World Paradigm was first introduced as an experimental paradigm relating eye-tracking data to lexical activation. This was done by measuring the location of a participant's visual fixation in real time in response to a spoken word. Shortly following this was the introduction of the ``proportion of fixations" method, whereby indicators of fixations in the VWP were aggregated across trials at dense time slices, creating an ostensible curve to help visualize and quantify the time course of lexical activation. In 2017, methods were introduced to make temporal analysis of these curves tractable via bootstrapping and a novel $\alpha$ correction to counteract the multiple comparison problems implicit in densely sampled time series. 

This dissertation contributes to the field in multiple ways. First, we reintroduce the \xt{bdots} package that significantly enhances researchers' ability to draw conclusions from VWP studies. Accompanying this are drastic changes to the underlying methodology to accommodate a far more robust set of assumptions regarding data generation. And finally, we introduce a generative model for relating eye mechanics to lexical activation alongside a novel look onset method that seeks to replace the proportion of fixation method currently in use.
}

\beforepreface
\afterpreface




















