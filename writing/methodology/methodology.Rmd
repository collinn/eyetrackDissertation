---
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.align = 'center')
```


# Outline

stuff here later

# Start

The purpose of this document is to examine the methodology in the original bdots paper (cite) and propose a new method for both the construction of fitted curve confidence intervals, as well as identifying regions in which a significant difference between two functions may be observed. 

The original bdots was created in the context of analyzing aggregate eyetracking data in the visual world paradigm. Given the nature of eyetracking data, collected across a series of trials for each subject, there was an assumed temporal correlation between the model errors, suggesting the incorporation of a correction for autocorrelation in the subsequent test statistic. However, as shown in (Nolte 2022 lol), the construction of the data itself was problematic, casting doubt on the relevance of the proposed method.

We posit that such structures are better understood in the real of functional data analysis. With the use of contrived functions using basis splines, we (hopefully) show regions of difference between two curves are better determined with the use of a permutation test at each time point. Something something something this addresses the issue with multiple testing while additionally providing more power in detecting group differences. 

[[possibly this next part should have come first]]

Independent the autocorrelation assumption, there are also issues with the original bdots algorithm. In short, there is a desire to capture both within and between subject variability  in our estimates of the population level curve. The original algorithm is as follows:

  1. Fit a (nonlinear) function for each subject with autocorrelated errors, using model estimates and standard errors to construct a sampling distribution of the subject parameters under a large sample normality assumption

  2. Using the sampling distribution in (1), randomly draw a bootstrap estimate for each of the model parameters for every subject

  3. Find the mean bootstrap estimate across individuals, providing a mean bootstrap

  4. Use this mean bootstrap estimate to generate a population level curve


The first two steps of this algorithm do indeed capture within subject level variance, yet the use of each of these subjects parameter estimates in (3) fails to generate any between-subject variability. The result is a population curve (who's) variability tends to zero as the number of subjects increases. Rather, we propose alternate steps for (2) and (3)

  2. From the population of $n$ subjects within a group, choose $n$ *with replacement* and draw parameters from their estimated sampling distribution, constructed in (1)
  
  3. Find the mean of the parameter estimates from the selected subjects for an estimate of the population mean
  
## Empirical comparison of two methods
  
To demonstrate this difference, we show here a collection of 25 subjects, fitting to each of them a nonlinear function and collecting their parameter estimates. From this collection, we create an empirical sampling distribution for the group. From this, we will do two things
  
  1. Draw 1000 samples to demonstrate the distribution of othe curves
  2. Draw 25 samples to emulate individual subjects, with noise
  
From here, we will perform both the original and the updated algorithm and see which one sucks and which one doesn't.

## Permutation test for difference

While the aforementioned algorithm serves to estimate the distribution of the population parameters (thus assisting in the construction of meaningful confidence intervals), they do not provide any indication of meaningful difference between two groups. Original bdots suggested the use of a $t$-statistic at each of the timepoints collected from the original algorithm, after the collection of bootstrapped parameter values had been turned in to a collection of bootstrapped curves:

$$
T_t = \frac{\overline{p}_{1t} - \overline{p}_{2t}}{\sqrt{s^2_{1t} + s_{2t}^2}}
$$
Of course, following the original algorithm, the estimates for the standard error were completely wrong. To account for the highly serialized nature of these tests, an adjustment based on the autocorrelation was proposed to control the FWER.

Instead, we propose the use of permutation tests between the groups to account for what may be typical variability within a curve. By setting a null distribution against these, we can do stuff that is sort of  awesome but i need to go look at the textbook where i got it before going any further, but also, its been nearly 40 minutes now and I have quite a few words that have been typed. However, I do have some cool simulation to show

# Extended cases

Idk maybe, but maybe also this doesn't go here but instead goes in the "updates to bdots itself independent of the methodology" i think that this is probably correct.

# Conclusion

In short, we propose two major changes to the way bdots is done. First, we alter the bdots algorithm to better estimate between subject varaiability. We verified this to be the case by looking at an empirical distribution and then trying to reconstruct it with a sample of 25 made up people. In this case, the  estimate of the population distribution was far superiod to that of the original. 

Second, we do away with the assumption of autocorrelation given the functional nature of this data, instead opting to use permutation tests to examine differences between the two groups. Does this account for when they are paired? Yes, because we are still doing a t-test. Because everything in the world is actually a t-test. We examined this with artificially constructed SPLINES compeltely divorced from the nonsense way that vwp eyetracking has been done and demonstrate a more general robustness, allowing bdots to be used in variety of contexts outside of simply the vwp.





















